{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13261110,"sourceType":"datasetVersion","datasetId":8403355}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required packages\n!pip install -q torch transformers supervision\n!pip install -q git+https://github.com/facebookresearch/segment-anything-2.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T18:05:03.023994Z","iopub.execute_input":"2025-10-04T18:05:03.024758Z","iopub.status.idle":"2025-10-04T18:07:15.322466Z","shell.execute_reply.started":"2025-10-04T18:05:03.024731Z","shell.execute_reply":"2025-10-04T18:07:15.321508Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# CELL 1 (MODIFIED)\nprint(\"Installing packages...\")\n!pip install -q torch transformers supervision\n!pip install -q git+https://github.com/facebookresearch/segment-anything-2.git\nprint(\"Installation complete. Forcing kernel restart...\")\n\n# This line will crash and restart the kernel automatically\nimport os\nos.kill(os.getpid(), 9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T18:09:12.174188Z","iopub.execute_input":"2025-10-04T18:09:12.174833Z","execution_failed":"2025-10-04T18:11:38.964Z"}},"outputs":[{"name":"stdout","text":"Installing packages...\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# This line will crash and restart the kernel automatically\nimport os\nos.kill(os.getpid(), 9)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-04T18:07:14.953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport requests\nfrom PIL import Image\nimport supervision as sv\nfrom transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n\n# Corrected imports to use the actual package name\nfrom segment_anything_2.build_sam import build_sam2\nfrom segment_anything_2.predictor import Sam2Predictor\n\n# Configure device (use GPU if available)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\n# Configure annotators for visualization\nbox_annotator = sv.BoundingBoxAnnotator()\nmask_annotator = sv.MaskAnnotator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T17:44:44.254181Z","iopub.execute_input":"2025-10-04T17:44:44.254537Z","iopub.status.idle":"2025-10-04T17:44:44.288961Z","shell.execute_reply.started":"2025-10-04T17:44:44.254505Z","shell.execute_reply":"2025-10-04T17:44:44.287961Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1980210750.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Corrected imports to use the actual package name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msegment_anything_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_sam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_sam2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msegment_anything_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSam2Predictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'segment_anything_2'"],"ename":"ModuleNotFoundError","evalue":"No module named 'segment_anything_2'","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"# --- Load GroundingDINO Model ---\ngrounding_dino_processor = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-base\")\ngrounding_dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(\"IDEA-Research/grounding-dino-base\").to(DEVICE)\n\n# --- Load SAM 2 Model ---\n# Note: This might take a moment as it downloads the model checkpoint\nsam2_checkpoint_url = \"https://dl.fbaipublicfiles.com/segment_anything_2/032424/sam2_hiera_base_plus.pth\"\nsam2_model = build_sam2(\n    model_id=\"sam2_hiera_b+\",\n    image_size=1024, # The image size the model was trained on\n    checkpoint_url=sam2_checkpoint_url\n).to(DEVICE)\n\nsam2_predictor = Sam2Predictor(sam2_model)\n\nprint(\"Models loaded successfully! âœ…\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Define Inputs\nIMAGE_URL = \"/kaggle/input/babyyy/ChatGPT Image Aug 22 2025 09_44_07 PM.png\"\nTEXT_PROMPT = \"a baby\"\nBOX_THRESHOLD = 0.35 # Confidence threshold for detected boxes\n\n# Load the image from the URL\nresponse = requests.get(IMAGE_URL, stream=True)\nimage_pil = Image.open(response.raw).convert(\"RGB\")\n\n# 2. Convert Text to Region Seeds (via GroundingDINO)\n# Pre-process the image and text\ninputs = grounding_dino_processor(images=image_pil, text=TEXT_PROMPT, return_tensors=\"pt\").to(DEVICE)\n\n# Run inference\nwith torch.no_grad():\n    outputs = grounding_dino_model(**inputs)\n\n# Post-process the results to get bounding boxes\nresults = grounding_dino_processor.post_process_grounded_object_detection(\n    outputs,\n    inputs.input_ids,\n    box_threshold=BOX_THRESHOLD,\n    text_threshold=BOX_THRESHOLD,\n    target_sizes=[image_pil.size[::-1]]\n)\n\n# Extract detected boxes\n# The output is a list of tuples, one for each image in the batch\ndetections = sv.Detections.from_transformers(results[0])\nprint(f\"Found {len(detections)} boxes for the prompt '{TEXT_PROMPT}'\")\n\n# 3. Feed Seeds to SAM 2\n# Set the image for the SAM 2 predictor\nsam2_predictor.set_image(image_pil)\n\n# Convert bounding boxes to the format required by SAM 2\ninput_boxes = detections.xyxy\n\n# Get segmentation masks from SAM 2\n# The model returns masks, quality scores, and low-res logits\nmasks, scores, logits = sam2_predictor.predict(\n    box=input_boxes,\n    multimask_output=False # We want one high-quality mask per box\n)\n\n# Add masks to our supervision Detections object\ndetections.mask = masks.cpu().numpy()\n\n# 4. Display the Final Mask Overlay\n# Annotate the image with both boxes and masks\nannotated_image = box_annotator.annotate(scene=image_pil.copy(), detections=detections)\nannotated_image = mask_annotator.annotate(scene=annotated_image, detections=detections)\n\nprint(\"\\nDisplaying final result... ðŸŽ¨\")\nsv.plot_image(annotated_image, size=(8, 8))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}